{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-19b9d0f78bb111c0",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Required Assignment 15.4: Comparing Gradient Descent with Stochastic Gradient Descent\n",
    "\n",
    "**Expected Time = 60 minutes** \n",
    "\n",
    "**Total Points = 40** \n",
    "\n",
    "This activity focuses on comparing the convergence and speed of gradient descent and stochastic gradient descent.  First, you will use a synthetic dataset to explore the convergence with naive implementations.  \n",
    "\n",
    "#### Index\n",
    "\n",
    "- [Problem 1](#-Problem-1)\n",
    "- [Problem 2](#-Problem-2)\n",
    "- [Problem 3](#-Problem-3)\n",
    "- [Problem 4](#-Problem-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, SGDRegressor\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-be8b62e3d7b62711",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Functions and Data\n",
    "\n",
    "Below you are given an `mse` and `df` functions for computing the gradient descent update.  Also, a simple linear dataset is built around the line $y = 3x$ with gaussian noise added.  You are to explore the convergence behavior of both stochastic gradient descent and gradient descent to the \"true\" coefficient $\\beta_0 = 3$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#synthetic dataset y = 3x\n",
    "np.random.seed(42)\n",
    "X = pd.DataFrame(np.linspace(0, 10, 100))\n",
    "y = 3*X + np.random.normal(scale = 5, size = 100).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ccfe6657c35d16e4",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "#mean squared error function\n",
    "def mse(theta, X = X, y = y):\n",
    "    return np.mean((y - theta*X)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b49bc7148a0a7e0f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "#derivative function for mse\n",
    "def df(theta, X = X, y = y):\n",
    "    return (mse(theta + 0.001, X = X, y = y) - mse(theta,X = X, y = y))/0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset and \"true\" line\n",
    "xs = np.linspace(0, 10, 100)\n",
    "plt.scatter(X, y)\n",
    "plt.plot(xs, 3*xs, color = 'red', label = r'Trend: $y = 3x$')\n",
    "plt.title('Synthetic Dataset')\n",
    "plt.grid()\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e09b4c3c14c52153",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#-Index)\n",
    "\n",
    "### Problem 1\n",
    "\n",
    "#### Gradient Descent\n",
    "\n",
    "**10 Points**\n",
    "\n",
    "Below, complete the update to determine `theta_next` using gradient descent and a step size of 0.001. Store the fist updated value `theta_next` inside the list `thetas`.\n",
    "\n",
    "Note to leave the `%time` magic command that will time your final execution of the loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-818ef45f42cc4d9c",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "%time\n",
    "thetas = [5]\n",
    "for i in range(200):\n",
    "    # complete the gradient descent update\n",
    "    theta_next = ''\n",
    "    #thetas.append(theta_next.values[0])\n",
    "    \n",
    "### BEGIN SOLUTION\n",
    "thetas = [5]\n",
    "for i in range(200):\n",
    "    # complete the gradient descent update\n",
    "    theta_next = thetas[-1] - 0.001*df(theta = thetas[-1])\n",
    "    #thetas.append(theta_next.values[0])\n",
    "    thetas.append(theta_next)\n",
    "### END SOLUTION\n",
    "\n",
    "### ANSWER CHECK\n",
    "print(thetas[-5:])\n",
    "plt.plot(thetas[::10], '--o')\n",
    "plt.axhline(3, color = 'red')\n",
    "plt.grid()\n",
    "plt.title('Gradient Descent with step size 0.001');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c6ca5c49c53c9cd9",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#-Index)\n",
    "\n",
    "### Problem 2\n",
    "\n",
    "#### SGD implementation\n",
    "\n",
    "**10 Points**\n",
    "\n",
    "Below, complete the gradient descent update using SGD where the sample is determined by the variable `x_sample`.  Use this and `y_sample` to compute the update and append this to `thetas_sgd`. Note the execution time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-fcc1c633e7cc540b",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "%time\n",
    "thetas_sgd = [5]\n",
    "for i in range(200):\n",
    "    x_sample = X.sample(1, random_state = i)\n",
    "    y_sample = y.iloc[x_sample.index.values]\n",
    "    #complete the gradient descent update\n",
    "    \n",
    "    \n",
    "    #thetas_sgd.append(theta_next.values[0])\n",
    "    \n",
    "### BEGIN SOLUTION\n",
    "thetas_sgd = [5]\n",
    "for i in range(200):\n",
    "    x_sample = X.sample(1, random_state = i)\n",
    "    y_sample = y.iloc[x_sample.index.values]\n",
    "    x_next = thetas_sgd[-1] - 0.001*df(theta = thetas_sgd[-1], X = x_sample, y = y_sample)\n",
    "    thetas_sgd.append(x_next)\n",
    "### END SOLUTION\n",
    "\n",
    "### ANSWER CHECK\n",
    "plt.plot(thetas_sgd[::10], '--o', label = 'SGD')\n",
    "plt.axhline(3, color = 'red')\n",
    "plt.grid()\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b324c096fcf3475a",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#-Index)\n",
    "\n",
    "### Problem 3\n",
    "\n",
    "#### Comparing Convergence\n",
    "\n",
    "**10 Points**\n",
    "\n",
    "Below, examine the plot comparing our SGD vs. Gradient Descent implementations.  Which of the following seems to be true?\n",
    "\n",
    "```\n",
    "a. Both methods converge and took similar time\n",
    "b. Both methods converge but SGD is much faster\n",
    "c. Neither method converged\n",
    "d. Only SGD converged.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(thetas_sgd[::10], '--o', label = 'SGD')\n",
    "plt.plot(thetas[::10], '--o', label = 'Gradient Descent')\n",
    "plt.axhline(3, color = 'red')\n",
    "plt.grid()\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2a7394f4dd2243da",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "ans3 = ''\n",
    "### BEGIN SOLUTION\n",
    "ans3 = 'a'\n",
    "### END SOLUTION\n",
    "\n",
    "### ANSWER CHECK\n",
    "print(ans3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a2e532c95d4a05ab",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#-Index)\n",
    "\n",
    "### Problem 4\n",
    "\n",
    "#### Why isn't `SGD` faster?\n",
    "\n",
    "**10 Points**\n",
    "\n",
    "It may have surprised you that using the SGD approach did not result in much of a speed difference.  This is mainly due to the size of the array and scale of the problem.  As stated in the scikitlearn `SGDRegressor` user guide [here](https://scikit-learn.org/stable/modules/sgd.html#regression), SGD will usually require a larger number of observations to be noticeably faster.  How many samples does `sklearn` suggest this is?\n",
    "\n",
    "```\n",
    "a. > 1000\n",
    "b. > 10,000\n",
    "c. > 100,000\n",
    "d. > 1,000,000\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-fc52f1b5c0b91797",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "ans4 = ''\n",
    "### BEGIN SOLUTION\n",
    "ans4 = 'b'\n",
    "### END SOLUTION\n",
    "\n",
    "### ANSWER CHECK\n",
    "print(ans4)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
