{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8lBFNC7Zq4uz",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b932fb3113b39867",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Required Assignment 15.2: Gradient Descent with Two Features\n",
    "\n",
    "**Expected Time = 60 minutes**\n",
    "\n",
    "**Total Points = 40**\n",
    "\n",
    "This activity focuses on using gradient descent with two features to find the optimal parameters for a regression model.  You will use the formulas for the gradients given in the lecture together with a small synthetic dataset to explore building a regression model with two variables.\n",
    "\n",
    "#### Index\n",
    "\n",
    "- [Problem 1](#-Problem-1)\n",
    "- [Problem 2](#-Problem-2)\n",
    "- [Problem 3](#-Problem-3)\n",
    "- [Problem 4](#-Problem-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3V9lq7d0rGS6"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sympy as sy\n",
    "from mpl_toolkits import mplot3d\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4b669aae15856864",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### The Data\n",
    "\n",
    "Below, a simple dataset is created around the line $y = 4.1x + 12.5 + \\epsilon$ where $\\epsilon$ are randomly generated values drawn from a Normal distribution $N(0, 2)$.  This means we want our model to uncover something close to $\\theta_0 = 12.5$ and $\\theta_1 = 4.1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M01y7YoGru-s"
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "x = np.linspace(0, 5, 100)\n",
    "y = 12.5 + 4.1*x + np.random.normal(size = 100, scale = 2)\n",
    "plt.scatter(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b6782a11323e9040",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#-Index)\n",
    "\n",
    "### Problem 1\n",
    "\n",
    "#### Adding a Bias Term\n",
    "\n",
    "**10 Points**\n",
    "\n",
    "Your two variable model will use a column of ones to stand in as the multiplier of $\\theta_0$.  Create a DataFrame with columns `['bias', 'x']` that contains a column of ones and the data `x`. Assign the result to the variable `X`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-dafb50ab2f85f477",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "\n",
    "X = ''\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "X = np.concatenate((np.ones((len(x), 1)), x.reshape(-1, 1)), axis = 1)\n",
    "X = pd.DataFrame(X, columns = ['bias', 'x'])\n",
    "### END SOLUTION\n",
    "\n",
    "# Answer check\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a3b21c26fca83102",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#-Index)\n",
    "\n",
    "### Problem 2\n",
    "\n",
    "#### Gradient of MSE\n",
    "\n",
    "**10 Points**\n",
    "\n",
    "Complete the function `mse_grad` below.  This function takes as input:\n",
    "\n",
    "- `theta`: A NumPy array containing the two parameters of the linear regression model.\n",
    "- `x`: A pandas DataFrame containing the input features.\n",
    "- `y`: A pandas Series or numpy array containing the target values.\n",
    "\n",
    "This function should extract the first and second columns of the input DataFrame `x` into variables `x0` and `x1`, respectively.\n",
    "\n",
    "Then, the function should calculate the partial derivatives with respect to `theta[0]` and `theta[1]` according to the formulas:\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\theta_0} l(\\overrightarrow{\\theta}, \\overrightarrow{x}, y_i) = 2(y_i - \\theta_0x_0 - \\theta_1x_1)(-x_0)$$\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\theta_1} l(\\overrightarrow{\\theta}, \\overrightarrow{x}, y_i) = 2(y_i - \\theta_0x_0 - \\theta_1x_1)(-x_1)$$\n",
    "\n",
    "Assign these derivatives to `dt0` and ` dt1`, respectively.\n",
    "\n",
    "You function should return a NumPy array with elements `dt0` and ` dt1`.\n",
    "\n",
    "\n",
    "Remember to find the mean of the gradient array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b77154523852536f",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "\n",
    "def mse_grad(theta, x, y):\n",
    "    return ''\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "def mse_grad(theta, x, y):\n",
    "    x0 = x.iloc[:, 0]\n",
    "    x1 = x.iloc[:, 1]\n",
    "    dt0 = np.mean(-2*(y - theta[0]*x0 - theta[1]*x1)*x0)\n",
    "    dt1 = np.mean(-2*(y - theta[0]*x0 - theta[1]*x1)*x1)\n",
    "    return np.array([dt0, dt1])\n",
    "### END SOLUTION\n",
    "\n",
    "# Answer check\n",
    "mse_grad(np.array([0, 0]), X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b4f29c3801f19335",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#-Index)\n",
    "\n",
    "### Problem 3\n",
    "\n",
    "#### Gradient Descent\n",
    "\n",
    "**10 Points**\n",
    "\n",
    "Use the initial value for `theta` and a learning rate of `lr = 0.01` to perform 1000 iterations of gradient descent.  Keep track of the updated array of theta as `thetas` below.  Recall the gradient descent formula as:\n",
    "\n",
    "$$\\theta_{i + 1} = \\theta_{i} - lr*grad(mse(\\theta_{i}))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b6083479fed0a1ab",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "thetas = []\n",
    "theta = ''\n",
    "lr = 0.01\n",
    "for i in range(1000):\n",
    "    #track theta\n",
    "    \n",
    "    #update theta\n",
    "    pass\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "theta = np.array([0, 0])\n",
    "thetas = []\n",
    "lr = 0.01\n",
    "for i in range(1000):\n",
    "    thetas.append(theta)\n",
    "    theta = theta - lr*mse_grad(theta, X, y)\n",
    "### END SOLUTION\n",
    "\n",
    "# Answer check\n",
    "thetas[-3:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c90318f523fd9430",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#-Index)\n",
    "\n",
    "### Problem 4\n",
    "\n",
    "#### DataFrame of updates\n",
    "\n",
    "**10 Points**\n",
    "\n",
    "Below, create a dataframe `thetas_df` that contains the value stored in `thetas`. Name the columns of this dataframe `intercept` and `slope`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-10ecc5283b1c2461",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "\n",
    "thetas_df = ''\n",
    "### BEGIN SOLUTION\n",
    "thetas_df = pd.DataFrame(thetas, columns = ['intercept', 'slope'])\n",
    "### END SOLUTION\n",
    "\n",
    "# Answer check\n",
    "thetas_df.tail(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ddd9c146b380fba1",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "#### Comparing with `sklearn`\n",
    "\n",
    "Below, a regression model from sklearn is fit and the coefficients are shown.  These should be very close to what your gradient descent routine found.  To get closer feel free to experiment with the learning rate and number of iterations.  Beware that too many iterations and too small a learning rate will be very slow to train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression(fit_intercept=False).fit(X, y)\n",
    "lr.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "colab": {
   "collapsed_sections": [],
   "name": "coding_assignment_15.7.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
