{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9acf6352dbcac84b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Required Assignment 13.3: End-to-End Classification Problem\n",
    "\n",
    "**Expected Time = 90 minutes**\n",
    "\n",
    "**Total Points = 85**\n",
    "\n",
    "This example leads you through an end-to-end analysis of a classification algorithm using `LogisticRegression`. You will perform some brief exploratory data analysis (EDA). Then, you will construct a feature engineering, selection, and model pipeline. Finally, you will explore the mistakes your models make and compare different classification metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b3e2ccac5f6adfa6",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Index\n",
    "\n",
    "- [Problem 1](#-Problem-1)\n",
    "- [Problem 2](#-Problem-2)\n",
    "- [Problem 3](#-Problem-3)\n",
    "- [Problem 4](#-Problem-4)\n",
    "- [Problem 5](#-Problem-5)\n",
    "- [Problem 6](#-Problem-6)\n",
    "- [Problem 7](#-Problem-7)\n",
    "- [Problem 8](#-Problem-8)\n",
    "- [Problem 9](#-Problem-9)\n",
    "- [Problem 10](#-Problem-10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn.compose import make_column_selector, make_column_transformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, PolynomialFeatures\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, RocCurveDisplay\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-65428383f8d4e2cb",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### The Data\n",
    "\n",
    "This data is originally from the IBM and contains information on a telecommunications company customer subscriptions.  Your task is to predict the customers who will Churn.  The data is loaded, displayed, and split below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/wa_churn.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TotalCharges'] = df['TotalCharges'].replace(' ', np.nan).astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df.drop(['Churn', 'customerID'], axis = 1), df['Churn'], random_state = 442,\n",
    "                                                   stratify = df['Churn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6a923be86a9c3493",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#-Index)\n",
    "\n",
    "### Problem 1\n",
    "\n",
    "### `make_column_selector`\n",
    "\n",
    "**5 Points**\n",
    "\n",
    "To begin, you may want to incorporate many of the categorical features here.  Rather than writing a list of names, you can use the `make_column_selector` to select features by datatype.  For example:\n",
    "\n",
    "```python\n",
    "make_column_selector(dtype_include=object)\n",
    "```\n",
    "\n",
    "will select all columns with `object` datatype.  This selector will replace the list of column names in the `make_column_transformer`.  \n",
    "\n",
    "Create a selector object to select the columns with `object` datatype below.  Assign this to `selector`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1fe59f12bdc326f6",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "\n",
    "selector = ''\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "selector = make_column_selector(dtype_include=object)\n",
    "### END SOLUTION\n",
    "\n",
    "# Answer check\n",
    "selector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-23116b197467f40a",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#-Index)\n",
    "\n",
    "### Problem 2\n",
    "\n",
    "#### Column Transformer\n",
    "\n",
    "**5 Points**\n",
    "\n",
    "Use the `make_column_transformer` function on the the columns selected by `selector`. To these columns, apply the `OneHotEncoder` with `drop = first`. To the `remainder` columns, apply `StandardScaler()`\n",
    "\n",
    "Assign the result to `transformer` below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0372451653165f0a",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "transformer = ''\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "transformer = make_column_transformer((OneHotEncoder(drop = 'first'), selector),\n",
    "                                     remainder = StandardScaler())\n",
    "### END SOLUTION\n",
    "\n",
    "# Answer check\n",
    "transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-bf504d4cb791ae15",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#-Index)\n",
    "\n",
    "### Problem 3\n",
    "\n",
    "#### Feature Extractor\n",
    "\n",
    "**5 Points**\n",
    "\n",
    "Just as in our earlier assignment you can use `LogisticRegression` with `l1` penalty to select features for the model.\n",
    "\n",
    "Below, create a `SelectFromModel` object that uses a `LogisticRegression` estimator with `penalty = 'l1'`, solver of `liblinear` and `random_state = 42`.  Assign your transformer as `extractor` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-82400b7f8a627da8",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "extractor = ''\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "extractor = SelectFromModel(LogisticRegression(penalty='l1', solver = 'liblinear' ,random_state = 42))\n",
    "### END SOLUTION\n",
    "\n",
    "# Answer check\n",
    "extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-fad1cb54d05ebe86",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#-Index)\n",
    "\n",
    "### Problem 4\n",
    "\n",
    "#### Pipeline with Estimator\n",
    "\n",
    "**10 Points**\n",
    "\n",
    "Now, build a pipeline `lgr_pipe` with named steps `transformer`, `selector`, and `lgr` that implement the column transformer, feature selector from above and a `LogisticRegression` estimator with `random_state = 42` and `max_iter = 1000`.  \n",
    "\n",
    "Fit the pipeline on the training data and determine the score on the test data.  \n",
    "\n",
    "Finally, use the function `score` to calculate the accuracy as a float to `pipe_1_acc` below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1d39cc69e0df01ef",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "\n",
    "lgr_pipe = ''\n",
    "pipe_1_acc = ''\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "lgr_pipe = Pipeline([('transformer', transformer),\n",
    "                    ('selector', extractor),\n",
    "                    ('lgr', LogisticRegression(random_state=42, max_iter = 1000))])\n",
    "\n",
    "lgr_pipe.fit(X_train, y_train)\n",
    "\n",
    "pipe_1_acc = lgr_pipe.score(X_test, y_test)\n",
    "### END SOLUTION\n",
    "\n",
    "# Answer check\n",
    "lgr_pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-fcbe9666a3ae1c13",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#-Index)\n",
    "\n",
    "### Problem 5\n",
    "\n",
    "#### Comparison to Baseline\n",
    "\n",
    "**10 Points**\n",
    "\n",
    "Use the `value_counts` method to determine the baseline score by choosing the majority class as your predictions.  Did your pipeline outperform the baseline model?  Answer `yes` or `no` as a string to `ans5` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-15e4440fbf556823",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "\n",
    "ans5 = ''\n",
    "### BEGIN SOLUTION\n",
    "ans5 = 'yes'\n",
    "# y_test.value_counts(normalize = True)\n",
    "### END SOLUTION\n",
    "\n",
    "### ANSWER TEST\n",
    "print(ans5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0c3f3ff64655c2c3",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#-Index)\n",
    "\n",
    "### Problem 6\n",
    "\n",
    "### Confusion Matrix and ROC Curve\n",
    "\n",
    "**10 Points**\n",
    "\n",
    "Examine both the confusion matrix and ROC curve using the cell below.  \n",
    "\n",
    "Create a 1 row by 2 column subplot object and place the confusion matrix on the left and ROC curve on the right.\n",
    "\n",
    "\n",
    "\n",
    "Use these to determine the number of false positives and false negatives on the test data.  Assign as an integer to `fp` and `fn` below.  Also, use the `RocCurveDisplay` legend to determine the AUC score.  Assign this as a float with two decimal places to `auc` below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ad705c09fe7ce1da",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "\n",
    "fp = ''\n",
    "fn = ''\n",
    "auc = ''\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "from sklearn.metrics import auc as skl_auc\n",
    "\n",
    "# fp = 126\n",
    "# fn = 194\n",
    "# auc = 0.86\n",
    "\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "fig.set_size_inches(12, 12)\n",
    "\n",
    "preds = lgr_pipe.predict(X_test)\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, preds)\n",
    "disp1 = ConfusionMatrixDisplay(conf_matrix)\n",
    "disp1.plot(ax=ax[0])\n",
    "\n",
    "# ROC Curve\n",
    "y_score = lgr_pipe.predict_proba(X_test)[:, 1]\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_score, pos_label='Yes')\n",
    "roc_auc_value = skl_auc(fpr, tpr)\n",
    "\n",
    "disp2 = RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc_value)\n",
    "disp2.plot(ax=ax[1])\n",
    "\n",
    "fp = conf_matrix[0][1]\n",
    "fn = conf_matrix[1][0]\n",
    "auc = round(roc_auc_value, 2)\n",
    "### END SOLUTION\n",
    "\n",
    "### ANSWER CHECK\n",
    "fp, fn, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-41fd81231a6479cd",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#-Index)\n",
    "\n",
    "### Problem 7\n",
    "\n",
    "#### What Really Matters\n",
    "\n",
    "**10 Points**\n",
    "\n",
    "You see above that you should have 194 False Negatives and 126 False Positives.  Suppose you want to implement an intervention to attempt turning over customers.  To use your classifier, this means being sure about targeting the customers you expect to churn -- in other words minimize the False Negatives.  Use the `predict_proba` method to select the probabilities of the `No` class.  Assign this as an array to `no_probs` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-94f1952bc6b3efc7",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "\n",
    "no_probs = ''\n",
    "### BEGIN SOLUTION\n",
    "no_probs = lgr_pipe.predict_proba(X_test)[:, 0]\n",
    "### END SOLUTION\n",
    "\n",
    "### ANSWER CHECK\n",
    "no_probs[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7712329fefdccfc9",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#-Index)\n",
    "\n",
    "### Problem 8\n",
    "\n",
    "#### Only target customers with high probability\n",
    "\n",
    "**10 Points**\n",
    "\n",
    "Even though our classifier is doing better than the baseline, it is still making a high number of mistakes.  Let's only look at the labels for `No` where you are better than 80% sure they are `No`'s.  Select these from your `no_probs` and assign as an array to `high_prob_no` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-40e806a8ce6d6bc5",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "\n",
    "high_prob_no = ''\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "high_prob_no = no_probs[no_probs > 0.8]\n",
    "### END SOLUTION\n",
    "\n",
    "### ANSWER CHECK\n",
    "high_prob_no[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b84378f3135a129a",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#-Index)\n",
    "\n",
    "### Problem 9\n",
    "\n",
    "#### Number of Predictions\n",
    "\n",
    "**10 Points**\n",
    "\n",
    "How many datapoints had probability greater than 80% of `No`?  What percent of the test data is this?  What percent of the original test data set `No` values is this?  Assign your answer as a float to `percent_of_test_data` and `percent_of_no` below. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-437af83090066d07",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "\n",
    "percent_of_test_data = ''\n",
    "percent_of_no = ''\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "percent_of_test_data = len(high_prob_no)/len(y_test)\n",
    "percent_of_no = len(high_prob_no)/sum(y_test == 'No')\n",
    "\n",
    "### END SOLUTION\n",
    "\n",
    "### ANSWER CHECK\n",
    "print(percent_of_test_data)\n",
    "print(percent_of_no)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-144cbb56d12e4bb6",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#-Index)\n",
    "\n",
    "### Problem 10\n",
    "\n",
    "#### Important Features\n",
    "\n",
    "**10 Points**\n",
    "\n",
    "Now, let us explore the coefficients of the model.  Because the data were scaled, we can think about the coefficients as speaking to a relative feature importance.  Extract the coefficients from your model and sort their absolute values from greatest to least.  Create a DataFrame called `coef_df` that contains the feature name and coefficient. The results begin as shown below:\n",
    "\n",
    "<table border=\"1\" class=\"dataframe\">  <thead>    <tr style=\"text-align: right;\">      <th></th>      <th>feature</th>      <th>coefs</th>    </tr>  </thead>  <tbody>    <tr>      <th>14</th>      <td>Contract_Two year</td>      <td>1.321160</td>    </tr>    <tr>      <th>20</th>      <td>tenure</td>      <td>1.301754</td>    </tr>    <tr>      <th>9</th>      <td>TechSupport_No internet service</td>      <td>0.753071</td>    </tr>    <tr>      <th>13</th>      <td>Contract_One year</td>      <td>0.701108</td>    </tr>    <tr>      <th>5</th>      <td>InternetService_Fiber optic</td>      <td>0.679121</td>    </tr>  </tbody></table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c0863b72b0f9f961",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "\n",
    "coef_df = ''\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "feature_names = lgr_pipe.named_steps['transformer'].get_feature_names_out() \n",
    "selected_features =feature_names[ [int(i[1:]) for i in lgr_pipe.named_steps['selector'].get_feature_names_out()]]\n",
    "clean_names = [i.split('__')[-1] for i in selected_features]\n",
    "coef_df = pd.DataFrame({'feature': clean_names, 'coefs': lgr_pipe.named_steps['lgr'].coef_[0]})\n",
    "coef_df['coefs'] = coef_df['coefs'].apply(abs)\n",
    "coef_df = coef_df.sort_values(by = 'coefs', ascending = False)\n",
    "### END SOLUTION\n",
    "\n",
    "### ANSWER CHECK\n",
    "coef_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7159e6a2a7b15966",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Notice that you should have a higher percentage of No values in your predictions than how much of the data it is comprised of.  In other words, if you randomly selected 50% of the data, you would expect 50% of the No.  Here, by ranking our predictions by probabilities and only selecting those with higher probability we are able to identify almost 70% of the No.  This notion of *LIFT* is an alternative method to that of ROC for understanding the quality of predictions, particularly if you have finite resources to expend.  If you are interested read more [here](https://www.ibm.com/docs/en/spss-statistics/24.0.0?topic=overtraining-cumulative-gains-lift-charts) and the `skplot` library has a straightforward visualization of the lift curve [here](https://scikit-plot.readthedocs.io/en/stable/metrics.html#scikitplot.metrics.plot_cumulative_gain)."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
