{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2aeab48733b78ac6",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Required Assignment 14.3: Pruning a Tree after Fitting\n",
    "\n",
    "**Expected Time = 60 minutes**\n",
    "\n",
    "**Total Points = 50**\n",
    "\n",
    "This activity focuses on using pruning to avoid overfitting in the decision tree.  By default, scikit-learn offers a `cost_complexity_pruning_path` that helps decide what the optimal `ccp_alpha` hyperparameter.  In what follows, you will learn how you can use the cost complexity paths to optimize the `ccp_alpha` hyperparameter in your `DecisionTreeClassifier`.  \n",
    "\n",
    "#### Index\n",
    "\n",
    "- [Problem 1](#-Problem-1)\n",
    "- [Problem 2](#-Problem-2)\n",
    "- [Problem 3](#-Problem-3)\n",
    "- [Problem 4](#-Problem-4)\n",
    "- [Problem 5](#-Problem-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier, export_text, plot_tree\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.compose import make_column_selector, make_column_transformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5665cbf09083ed93",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### The Data\n",
    "\n",
    "For this exercise, you will use the iris data from the videos.  Again, the goal is to predict the species of flower using measurements of the plant.  Below, the data is loaded, cleaned, and split for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = sns.load_dataset('iris')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris.drop(['species'], axis = 1)\n",
    "y = iris['species']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c83b2baba4b88b22",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#-Index)\n",
    "\n",
    "### Problem 1\n",
    "\n",
    "#### Extracting the path\n",
    "\n",
    "**10 Points**\n",
    "\n",
    "Below, fit a `DecisionTreeClassifier` on the training data `X_train` and `y_train` with all default parameters and `random_state = 42`.  \n",
    "\n",
    "Use the `.cost_complexity_pruning_path()` method on the estimator with arguments equal to the taining data. Assign the results to the variable `path` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-953e99bea7499f77",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "dtree = ''\n",
    "path = ''\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "dtree = DecisionTreeClassifier(random_state = 42).fit(X_train, y_train)\n",
    "path = dtree.cost_complexity_pruning_path(X_train, y_train)\n",
    "### END SOLUTION\n",
    "\n",
    "# Answer check\n",
    "path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-92da2a6846954329",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#-Index)\n",
    "\n",
    "### Problem 2\n",
    "\n",
    "#### The `ccp_alphas` and `impurities`\n",
    "\n",
    "**10 Points**\n",
    "\n",
    "Your `path` variable now contains a collection of `ccp_alpha` values that control the pruning and the associated impurities for each prune.  \n",
    "\n",
    "Assign the result of the `ccp_alpha` method on `path` to  the variable `ccp_alphas`. Use the `impurities` method on `path` and assign the result to the variable `impurities` below.  \n",
    "\n",
    "Note, the plot of the alphas again impurities demonstrates that the full tree has total impurity 0 and increases until the tree with only a single node.  Uncomment the code to visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-efb7a83d19aac452",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "\n",
    "dtree = ''\n",
    "path = ''\n",
    "ccp_alphas = ''\n",
    "impurities = ''\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "dtree = DecisionTreeClassifier(random_state = 42).fit(X_train, y_train)\n",
    "path = dtree.cost_complexity_pruning_path(X_train, y_train)\n",
    "ccp_alphas = path.ccp_alphas\n",
    "impurities = path.impurities\n",
    "### END SOLUTION\n",
    "\n",
    "## Answer check\n",
    "plt.step(ccp_alphas[:-1], impurities[:-1], '--o')\n",
    "plt.title('Impurity vs. Effective Alpha')\n",
    "plt.xlabel('Effective Alphas')\n",
    "plt.ylabel('Total Impurity of Leaves');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-999d30fa249e3034",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#-Index)\n",
    "\n",
    "### Problem 3\n",
    "\n",
    "#### How the Nodes change with alpha\n",
    "\n",
    "**10 Points**\n",
    "\n",
    "\n",
    "Now, loop over the `ccp_alphas` and build a `DecisionTreeClassifier` with `random_state = 42` and `ccp_alpha = i` where `i` is the given alpha. Fit each estimator to the training data. Inside the loop, keep track of the fit tree node count using the `.tree_.node_count` attribute and store it the list `nodes`. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-11ef6d3a8409c3c8",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "nodes = []\n",
    "for i in ccp_alphas:\n",
    "    #create decision tree and fit it\n",
    "    \n",
    "    #append node count to nodes\n",
    "    \n",
    "    pass\n",
    "### BEGIN SOLUTION\n",
    "nodes = []\n",
    "for i in ccp_alphas[:-1]:\n",
    "    dtree = DecisionTreeClassifier(random_state=42, ccp_alpha=i).fit(X_train, y_train)\n",
    "    nodes.append(dtree.tree_.node_count)\n",
    "### END SOLUTION\n",
    "\n",
    "## Answer check\n",
    "plt.step(ccp_alphas[:-1], nodes, '--o')\n",
    "plt.xlabel('Alpha')\n",
    "plt.ylabel('Nodes')\n",
    "plt.title('CCP Alpha vs. Number of Nodes')\n",
    "plt.grid();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-021effca8cb9c629",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#-Index)\n",
    "\n",
    "### Problem 4\n",
    "\n",
    "#### Depth of tree as alpha increases\n",
    "\n",
    "**10 Points**\n",
    "\n",
    "Repeat the above exercise this time extracting and tracking the depth of the tree using the `.get_depth()` method.  Uncomment the code to visualize the relationship.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-95c372d5e4b6c2d6",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "depths = []\n",
    "for i in ccp_alphas:\n",
    "    pass\n",
    "\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "depths = []\n",
    "for i in ccp_alphas:\n",
    "    dtree = DecisionTreeClassifier(random_state=42, ccp_alpha=i).fit(X_train, y_train)\n",
    "    depths.append(dtree.get_depth())\n",
    "### END SOLUTION\n",
    "\n",
    "## Answer check\n",
    "# plt.step(ccp_alphas, depths, '--o')\n",
    "# plt.ylabel('Depth of Tree')\n",
    "# plt.xlabel('Alpha')\n",
    "# plt.grid()\n",
    "# plt.title('Alpha vs. Depth of tree');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-21676d6e4dbab5d2",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#-Index)\n",
    "\n",
    "### Problem 5\n",
    "\n",
    "#### Scoring the trees\n",
    "\n",
    "**10 Points**\n",
    "\n",
    "Finally, loop over the `ccp_alphas` building a tree for each and recording its training and testing accuracy to `train_accs` and `test_accs` below.  What `ccp_alpha` value resulted in the highest accuracy on the test data?  Uncomment the code to view the plot of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a9cf5340209a5a58",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "\n",
    "train_accs = []\n",
    "test_accs = []\n",
    "for i in ccp_alphas[:-1]:\n",
    "    pass\n",
    "\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "train_accs = []\n",
    "test_accs = []\n",
    "for i in ccp_alphas[:-1]:\n",
    "    dtree = DecisionTreeClassifier(random_state=42, ccp_alpha=i).fit(X_train, y_train)\n",
    "    train_accs.append(dtree.score(X_train, y_train))\n",
    "    test_accs.append(dtree.score(X_test, y_test))\n",
    "### END SOLUTION\n",
    "\n",
    "# # Answer check\n",
    "plt.step(ccp_alphas[:-2], train_accs[:-1], '--o', label = 'Train')\n",
    "plt.step(ccp_alphas[:-2], test_accs[:-1], '--o', label = 'Test')\n",
    "plt.plot(ccp_alphas[np.argmax(test_accs)], max(test_accs), 'ro', markersize = 12, alpha = 0.4, label = 'Best Accuracy')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('Accuracy');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b21348a0f0e55229",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "For more examples of using Cost Complexity Pruning in scikitlearn see the example documentation [here](https://scikit-learn.org/stable/auto_examples/tree/plot_cost_complexity_pruning.html), and for more details on how the algorithm determines where to prune and how to compute alpha see [here](http://mlwiki.org/index.php/Cost-Complexity_Pruning).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2, figsize = (30, 10))\n",
    "plot_tree(DecisionTreeClassifier(random_state=42).fit(X_train, y_train), ax = ax[0], filled = True, feature_names=X.columns);\n",
    "ax[0].set_title('Unpruned Tree', fontsize = 20)\n",
    "plot_tree(DecisionTreeClassifier(random_state = 42, ccp_alpha=ccp_alphas[np.argmax(test_accs)]).fit(X_train, y_train), filled = True, feature_names = X.columns);\n",
    "ax[1].set_title('Pruned Tree', fontsize = 20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
